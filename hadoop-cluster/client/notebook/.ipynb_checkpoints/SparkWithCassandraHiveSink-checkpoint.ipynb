{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52a25db9-d8cc-4edf-968d-dc342da7b745",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cassandra + PySpark Batching 예제\n",
    "\n",
    "### 1. findspark를 통해 pyspark 등 라이브러리 추가, SparkContext 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc93428-bfa1-4a7d-8435-eff379310a08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/22 01:14:19 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.2.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.3.0.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.2.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.3.0.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/com.typesafe_config-1.4.1.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar added multiple times to distributed cache.\n",
      "23/03/22 01:14:25 WARN Client: Same path resource file:///root/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar added multiple times to distributed cache.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/local/lib/spark-3.3.2-bin-hadoop3\")\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import udf, col, from_json, pandas_udf, split\n",
    "\n",
    "sconf = SparkConf()\n",
    "sconf.setAppName(\"Jupyter_Notebook_2\").set(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.3.0\")\n",
    "\n",
    "sc = SparkContext(conf=sconf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571470e6-2614-4b1d-8879-49ae387a8289",
   "metadata": {},
   "source": [
    "### 2. SparkSession 시작, Cassandra와 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3240ba-c9f8-4577-9111-617b8cd09472",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = SparkSession(sc)\n",
    "\n",
    "cassandra_keyspace = \"tagmanager\"\n",
    "cassandra_table = \"stream\"\n",
    "\n",
    "batch_df = session.read \\\n",
    "      .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "  .option(\"checkpointLocation\", \"/\") \\\n",
    "  .option(\"spark.cassandra.connection.host\", \"master01\") \\\n",
    "  .option(\"spark.cassandra.connection.port\", 9042) \\\n",
    "  .option(\"keyspace\", cassandra_keyspace) \\\n",
    "  .option(\"table\", cassandra_table) \\\n",
    "  .option(\"spark.cassandra.connection.remoteConnectionsPerExecutor\", 10) \\\n",
    "  .option(\"spark.cassandra.output.concurrent.writes\", 1000) \\\n",
    "  .option(\"spark.cassandra.concurrent.reads\", 512) \\\n",
    "  .option(\"spark.cassandra.output.batch.grouping.buffer.size\", 1000) \\\n",
    "  .option(\"spark.cassandra.connection.keep_alive_ms\", 600000000) \\\n",
    "      .load()\n",
    "batch_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca858e34-8ec0-43fd-9adb-f999a345eecb",
   "metadata": {},
   "source": [
    "### 3. PySqark SQL을 이용해 쿼리 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0b94c3-99a8-42e0-b53b-af829ebbc4c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "# 간편한 between 연산을 위해 만든 유틸리티 함수\n",
    "# base_time: 기준 시간\n",
    "# interval: 기분 시간으로부터 얼마나 조회를 할 지의 범위\n",
    "# 초, 분, 시 등의 단위\n",
    "# ex. timestamp_range(\"2023-03-21 13:49:00\", 10, 'm') => 2023-03-21 13:49:00 부터 10분 이후의 시간까지\n",
    "def timestamp_range(base_time, interval, unit):\n",
    "    dt_obj = datetime.strptime(base_time, '%Y-%m-%d %H:%M:%S')\n",
    "    if unit=='s':\n",
    "        if interval>=0:\n",
    "            return (dt_obj, dt_obj+timedelta(seconds=interval))\n",
    "        else:\n",
    "            return (dt_obj-timedelta(seconds=-interval), dt_obj)\n",
    "    if unit=='m':\n",
    "        if interval>=0:\n",
    "            return (dt_obj, dt_obj+timedelta(minutes=interval))\n",
    "        else:\n",
    "            return (dt_obj-timedelta(minutes=-interval), dt_obj)\n",
    "    if unit=='H':\n",
    "        if interval>=0:\n",
    "            return (dt_obj, dt_obj+timedelta(hours=interval))\n",
    "        else:\n",
    "            return (dt_obj-timedelta(hours=-interval), dt_obj)\n",
    "    if unit=='D':\n",
    "        if interval>=0:\n",
    "            return (dt_obj, dt_obj+timedelta(days=interval))\n",
    "        else:\n",
    "            return (dt_obj-timedelta(days=-interval), dt_obj)\n",
    "    if unit=='M':\n",
    "        if interval>=0:\n",
    "            return (dt_obj, dt_obj+timedelta(months=interval))\n",
    "        else:\n",
    "            return (dt_obj-timedelta(months=-interval), dt_obj)\n",
    "    if unit=='Y':\n",
    "        if interval>=0:\n",
    "            return (dt_obj, dt_obj+timedelta(years=interval))\n",
    "        else:\n",
    "            return (dt_obj-timedelta(years=-interval), dt_obj)\n",
    "\n",
    "\n",
    "base_time = \"2023-03-22 08:23:00\"\n",
    "\n",
    "\n",
    "# 해당 시간 사이의 모든 데이터 조회\n",
    "batch_df.select(\"*\") \\\n",
    "    .where(col(\"creation_timestamp\") \\\n",
    "            .between(*timestamp_range(base_time, 1, 'm'))) \\\n",
    "    .show()\n",
    "\n",
    "# 해당 시간 사이에 http://localhost:3000/second에서 일어난 click 이벤트 조회\n",
    "batch_df.select(\"*\") \\\n",
    "    .where(col(\"creation_timestamp\") \\\n",
    "            .between(*timestamp_range(base_time, 1, 'm'))) \\\n",
    "    .where(col(\"location\") \\\n",
    "           .like(\"http://localhost:3000/second\")) \\\n",
    "    .where(col(\"event\") \\\n",
    "           .like(\"click\")) \\\n",
    "    .show()\n",
    "\n",
    "# 해당 시간 사이에 http://localhost:3000/second에 접속한 사용자 조회\n",
    "batch_df.select(\"*\") \\\n",
    "    .where(col(\"creation_timestamp\") \\\n",
    "            .between(*timestamp_range(base_time, -30, 'm'))) \\\n",
    "    .where(col(\"location\") \\\n",
    "            .like(\"http://localhost:3000/second\")) \\\n",
    "    .select(\"session_id\").distinct() \\\n",
    "    .show()\n",
    "\n",
    "# location, event 기준으로 그룹핑 후 개수 세기\n",
    "batch_df.select(\"*\") \\\n",
    "    .where(col(\"creation_timestamp\") \\\n",
    "            .between(*timestamp_range(base_time, 1, 'D'))) \\\n",
    "    .groupBy(\"location\", \"event\").count() \\\n",
    "    .show()\n",
    "\n",
    "# session_id 기준으로 해당 시간동안의 서비스 체류시간 연산\n",
    "batch_df.select(\"*\") \\\n",
    "    .where(col(\"creation_timestamp\") \\\n",
    "            .between(*timestamp_range(base_time, 1, 'D'))) \\\n",
    "    .groupBy(\"session_id\").agg( \\\n",
    "        max(\"creation_timestamp\").alias(\"service_leave\"), \\\n",
    "        min(\"creation_timestamp\").alias(\"service_enter\") \\\n",
    "     ).withColumn(\"duration\", (col(\"service_leave\")-col(\"service_enter\")).cast(\"long\")) \\\n",
    "    .show()\n",
    "\n",
    "# session_id 기준으로 해당 시간동안의 페이지 체류시간 연산\n",
    "batch_df.select(\"*\") \\\n",
    "    .where(col(\"creation_timestamp\") \\\n",
    "            .between(*timestamp_range(base_time, 1, 'D'))) \\\n",
    "    .groupBy(\"location\", \"session_id\").agg( \\\n",
    "        avg(\"page_duration\").alias(\"duration\")*0.001\n",
    "    ).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0c746e-13b4-4f08-be0f-d251537b5f3c",
   "metadata": {},
   "source": [
    "### 4. Hive와 연결 및 INSERT\n",
    "Hive 내 `test` DATABASE의 `weblogs` TABLE의 구조는 다음과 같다.\n",
    "```\n",
    "CREATE TABLE IF NOT EXISTS weblogs (\n",
    "creation_timestamp STRING,\n",
    "session_id STRING,\n",
    "client_id STRING,\n",
    "event STRING,\n",
    "key STRING,\n",
    "location STRING,\n",
    "position_x STRING,\n",
    "position_y STRING,\n",
    "service_token STRING,\n",
    "target_id STRING\n",
    ") PARTITIONED BY (service_id STRING)\n",
    "STORED AS ORC\n",
    "LOCATION 'hdfs:///user/hive/warehouse';\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0371e31e-b9bf-4a51-a366-4feb2cf26193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b43c9-e685-4fe1-9ee8-a60fc8efe06e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.stop()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc8d26c-5a04-42df-aa12-76696a07cc83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
